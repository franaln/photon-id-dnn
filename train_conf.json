{
    "layers": 6,
    "neurons": 256,
    "activation": "relu",
    "optimizer": "adam",
    "lr": 0.0001,
    "batch_size": 4096,
    "epochs": 100,
    "dropout": 0.3,
    "batch_norm": 0
}
