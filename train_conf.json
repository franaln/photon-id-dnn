{
    "layers": 6,
    "neurons": 256,
    "activation": "relu",
    "optimizer": "adam",
    "lr": 0.0001,
    "batch_size": 16384,
    "epochs": 200,
    "dropout": 0.3
}
